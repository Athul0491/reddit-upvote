# RedditEngage (CSP - Comment Score Predictor)

A distributed machine learning pipeline to predict Reddit comment popularity based on metadata and content, built using Apache Spark, Google Cloud, and Scikit-learn.

## ğŸ“¦ Dataset

We use a curated 23 GB subset of the "Reddit Comments/Submissions (2005â€“2024)" dataset (3.12TB), covering Jan 2009 to Dec 2011 (~37.6M comments). The data is stored in JSON format and hosted on Google Cloud Storage.

## âš™ï¸ Environment

- Apache Spark (Google Dataproc)
- Google Cloud CLI
- Python 3.x
- Jupyter Notebook
- Scikit-learn

## ğŸ“Š Methodology

### 1. Data Ingestion & Preprocessing
- Load JSON from GCS into Spark
- Remove `[deleted]`, `[removed]` comments & those with missing `ups`
- Feature engineering: subreddit, time of post, comment length, edit delay
- Binary classification label: **Popular = â‰¥5 upvotes**

### 2. Feature Extraction
- `RegexTokenizer` â†’ `HashingTF` â†’ `IDF` for text
- OneHot encode subreddit
- VectorAssembler for combining all features

### 3. Models
- **Spark MLlib**: Logistic Regression & Naive Bayes
- **From Scratch**: SGD-based Logistic Regression using RDDs
- **Scikit-learn (Driver Node)**: Logistic Regression + Naive Bayes with SHAP analysis and coefficient plots

### 4. Evaluation
- Metrics: Accuracy, Precision, Recall, F1, AUC
- Train-Test Split: 85/15

## ğŸ“ˆ Results

| Model                   | Accuracy | F1 Score | AUC   |
|------------------------|----------|----------|-------|
| Spark Logistic Reg     | 0.625    | 0.448    | 0.689 |
| Spark Naive Bayes      | 0.601    | 0.410    | 0.654 |
| Custom SGD LogisticReg | 0.613    | 0.433    | 0.674 |
| Sklearn Logistic Reg   | 0.640    | 0.450    | 0.706 |
| Sklearn Naive Bayes    | 0.618    | 0.417    | 0.669 |

## ğŸ” Insights

- Polite and positive language boosts upvotes.
- Toxic, dismissive tone leads to downvotes.
- r/science and r/AskReddit are more predictable than r/funny.
- SHAP & coefficient analysis helped validate key features.

## ğŸ§ª How to Run

1. Clone this repo
2. Set up a Google Cloud project and Dataproc cluster
3. Upload dataset to GCS
4. Run Spark jobs via `spark-submit` or notebooks
5. For Sklearn, sample data using `.sample(fraction=0.1)` from Spark to local node

# ğŸ“‚ Project Structure

```
redditengage/
â”œâ”€â”€ spark_pipeline.py          # End-to-end Spark MLlib pipeline (tokenization, TF-IDF, model training)
â”œâ”€â”€ sgd_from_scratch.py        # Custom logistic regression using SGD and RDDs  
â”œâ”€â”€ sklearn_diagnostics.ipynb  # SHAP analysis, error histograms, feature importance using Scikit-learn
â”œâ”€â”€ data_preprocessing.py      # Data cleaning and feature engineering steps
â”œâ”€â”€ dataproc_setup.sh          # CLI script to set up GCP bucket and Dataproc cluster
â”œâ”€â”€ requirements.txt           # Python dependencies for local sklearn diagnostics
â”œâ”€â”€ data/                      # Placeholder for .json dataset (stored in GCS)
â””â”€â”€ plots/                     # Output visualizations (bar plots, ROC, SHAP)
```

## ğŸ¤ Contributors

- Athul Thulasidasan
- Benyamin Tafreshian

## ğŸ“œ License

MIT License â€“ see `LICENSE` file for details.
